
# MD fitting

import os
os.getcwd()
os.chdir('XX')  
cwd = os.getcwd()
cwd


# In[2]:


import pandas as pd
import numpy as np
import xgboost as xgb
from xgboost.sklearn import XGBClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
#from xgboost import plot_importance
from sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score, average_precision_score, auc
from sklearn.metrics import roc_curve, roc_auc_score, classification_report, confusion_matrix, precision_recall_curve
from sklearn.impute import SimpleImputer
from imblearn.combine import SMOTEENN
from imblearn.under_sampling import RepeatedEditedNearestNeighbours, RandomUnderSampler, NearMiss, ClusterCentroids
from imblearn.over_sampling import SMOTE
from collections import Counter
#from pandas_profiling import ProfileReport
from matplotlib import pyplot
import matplotlib.pylab as plt
get_ipython().run_line_magic('matplotlib', 'inline')
import pickle


# In[3]:


# Read data
md1 = pd.read_pickle(r"AC.pickle")


# In[4]:


def func_Target(x):
#     if x["unbun"] == 1:
#         return 1
    # "UNBUN"
    if (x['cob']+x['mul_sur']+x['chf'])>0:
        return 1
    else:
        return 0
    # "NO_WA"
md1['FWA_IND'] = md1.apply(lambda x: func_Target(x), axis = 1)

# Remove unbun
indexes = md1[md1['unbun'] == 1].index
md1.drop(indexes, inplace = True)

# Before running below code, please make sure data is in OHE format
data_y=pd.DataFrame()
# FWA_IND
data_y = md1['FWA_IND'].copy()
drop_variable = ['A1']

data_x = md1.drop(drop_variable, axis = 1)

drop_list2 = [
'A2'
]

data_x = data_x.drop(drop_list2, axis = 1)


# #### Train test split with stratified option

# In[5]:


X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, stratify=data_y, random_state = 1234)


# In[6]:


print(y_train.value_counts())
print(y_test.value_counts())
print(sum(data_y)/data_y.shape[0])


# ### SMOTE implementation

# In[7]:


counter = Counter(y_train)
print('Original dataset shape %s' % counter)


# In[8]:


smote = NearMiss(sampling_strategy=2/8, version=3)
X_train1, y_train1 = smote.fit_resample(X_train, y_train)
counter1           = Counter(y_train1)
print('Undersampled dataset shape %s' % counter1)


# In[9]:


X_train_claim = X_train1.ClaimNumber
X_test_claim = X_test.ClaimNumber
X_train1 = X_train1.drop(['ClaimNumber'], axis=1)
X_test = X_test.drop(['ClaimNumber'], axis=1)


# ### XGBoost cross validation

# In[10]:


def modelfit(alg, cv_folds=5, early_stopping_rounds=50):
    
    xgb_param = alg.get_xgb_params()
    xgtrain = xgb.DMatrix(X_train1.values, label=y_train1.values)
    cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,
    metrics='aucpr', early_stopping_rounds=early_stopping_rounds, verbose_eval=False)
    alg.set_params(n_estimators=cvresult.shape[0])
    
    #Fit the algorithm on the data
    alg.fit(X_train1, y_train1, eval_metric='aucpr')
        
    #Predict training set:
    y_pred_train = alg.predict(X_train1)
    y_predprob_train = alg.predict_proba(X_train1)[:,1]
    
    #Predict test set:
    y_pred_test = alg.predict(X_test)
    y_predprob_test = alg.predict_proba(X_test)[:,1]
        
    #Print model report:
    print("\nModel Report for Training set")
    print("Accuracy (Train) : %.4g" % metrics.accuracy_score(y_train1.values, y_pred_train))
    print("AUC Score (Train): %f" % metrics.roc_auc_score(y_train1, y_predprob_train))
    print("Precision Score (Train): %f" % metrics.average_precision_score(y_train1, y_predprob_train))
    print("Recall Score (Train): %f" % metrics.recall_score(y_train1, y_pred_train))
    print("\nModel Report for Test set")
    print("Accuracy (Test) : %.4g" % metrics.accuracy_score(y_test.values, y_pred_test))
    print("AUC Score (Test): %f" % metrics.roc_auc_score(y_test, y_predprob_test))
    print("Precision Score (Test): %f" % metrics.average_precision_score(y_test, y_predprob_test))
    print("Recall Score (Test): %f" % metrics.recall_score(y_test, y_pred_test))
                    
    print(pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)[0:30])


# #### Initial model with fixed parameters

# In[10]:


xgb1 = XGBClassifier(
 learning_rate=0.1,
 n_estimators=100,
 max_depth=5,
 min_child_weight=1,
 gamma=0,
 subsample=0.8,
 colsample_bytree=0.8,
 objective= 'binary:logistic',
 nthread=4,
 scale_pos_weight=1,
 max_delta_step=0,
 reg_alpha=0,
 reg_lambda=1,
 seed=2)

modelfit(xgb1)


# #### Tune max_depth and min_child_weight

# In[11]:


param_test1 = {
 'max_depth':range(3,10,2),
 'min_child_weight':range(1,4,2)
}
gsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5, min_child_weight=1, 
                                                  gamma=0, subsample=0.8, colsample_bytree=0.8, objective='binary:logistic', 
                                                  nthread=4, scale_pos_weight=1, seed=2), 
                        param_grid=param_test1, scoring='f1', n_jobs=-1, cv=5)
gsearch1.fit(X_train1, y_train1)
gsearch1.best_params_, gsearch1.best_score_


# #### Tune gamma

# In[12]:


param_test2 = {
 'gamma':[i/10.0 for i in range(0,5)]
}
gsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=9, min_child_weight=3, 
                                                  gamma=0, subsample=0.8, colsample_bytree=0.8, objective='binary:logistic', 
                                                  nthread=4, scale_pos_weight=1, seed=2), 
                        param_grid=param_test2, scoring='f1', n_jobs=-1, cv=5)
gsearch1.fit(X_train1, y_train1)
gsearch1.best_params_, gsearch1.best_score_


# #### Use tuned parameters for an updated model

# In[13]:


xgb2 = XGBClassifier(
 learning_rate=0.1,
 n_estimators=100,
 max_depth=9,
 min_child_weight=3,
 gamma=0.1,
 subsample=0.8,
 colsample_bytree=0.8,
 objective='binary:logistic',
 nthread=4,
 scale_pos_weight=1,
 max_delta_step=0,
 reg_alpha=0,
 reg_lambda=1,
 seed=2)

modelfit(xgb2)


# #### Tune subsample and colsample_bytree

# In[14]:


param_test3 = {
 'subsample':[i/10.0 for i in range(6,10)],
 'colsample_bytree':[i/10.0 for i in range(6,10)]
}
gsearch3 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=9, min_child_weight=3, 
                                                  gamma=0.1, subsample=0.8, colsample_bytree=0.8, objective='binary:logistic', 
                                                  nthread=4, scale_pos_weight=1, seed=2), 
                        param_grid = param_test3, scoring='f1', n_jobs=-1, cv=5)
gsearch3.fit(X_train1, y_train1)
gsearch3.best_params_, gsearch3.best_score_


# #### Rerun model wih updated parameters

# In[15]:


xgb3 = XGBClassifier(
 learning_rate=0.1,
 n_estimators=100,
 max_depth=9,
 min_child_weight=3,
 gamma=0.1,
 subsample=0.9,
 colsample_bytree=0.7,
 objective='binary:logistic',
 nthread=4,
 scale_pos_weight=9,
 max_delta_step=1,
 reg_alpha=0,
 reg_lambda=1,
 seed=2)

modelfit(xgb3)


# In[11]:


xgb4 = XGBClassifier(
 learning_rate=0.1,
 n_estimators=100,
 max_depth=9,
 min_child_weight=1,
 gamma=0.3,
 subsample=0.9,
 colsample_bytree=0.7,
 objective='binary:logistic',
 nthread=4,
 scale_pos_weight=9,
 max_delta_step=1,
 reg_alpha=0,
 reg_lambda=1,
 seed=2)

xgb4.fit(X_train1, y_train1)


# ### Model results

# In[12]:


y_pred_probs = xgb4.predict_proba(X_test)[:,1]
y_pred = (y_pred_probs >= 0.5).astype(bool)

print('F1 Accuracy : {}'.format(f1_score(y_test, y_pred)))
print('Precision Score: {}'.format(precision_score(y_test, y_pred)))
print('Recall Score: {}'.format(recall_score(y_test, y_pred)))
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))


# In[13]:


#shapely values
import shap

# Need to load JS vis in the notebook
shap.initjs()


# In[14]:


explainer = shap.TreeExplainer(xgb4)
shap_values = explainer.shap_values(X_test)


# In[15]:


shap_values_df = pd.DataFrame(data = shap_values, columns = [w+'_shap' for w in X_test.columns])
shap_values_df.reset_index(drop = True, inplace = True)
X_test_claim.reset_index(drop = True, inplace = True)
y_test.reset_index(drop = True, inplace = True)
X_test.reset_index(drop = True, inplace = True)


# In[16]:


# Add predicted probabilities and fraud tags
shap_values_df['fwa_mapfre'] = y_test
shap_values_df['fwa_pred'] = (y_pred_probs >= 0.5).astype(int)
shap_values_df['fwa_prob'] = y_pred_probs
shap_values_df['claimnumber'] = X_test_claim


# In[17]:


shap_val_master = pd.concat([X_test, shap_values_df], axis = 1)


# In[18]:


shap_val_master1 = shap_val_master[(shap_val_master.fwa_pred == 1) & (shap_val_master.fwa_mapfre == 0)]


# In[19]:


print(shap_val_master.shape)
print(shap_val_master1.shape)


# In[20]:


shap_val_master1 = shap_val_master1.sort_values(by='fwa_prob', ascending=False)


# ## Plots

# In[21]:


j=5
pn = shap_val_master1.iloc[[j]].claimnumber.values[0]
pr = 100*shap_val_master1.iloc[[j]].fwa_prob.values[0]
shap_val = shap_val_master1[[w+'_shap' for w in X_test.columns]]
#     X_data = shap_val_master2[feature_names]
d = shap.decision_plot(explainer.expected_value, np.asarray(shap_val.iloc[[j]]), X_test.iloc[[j]], link='logit', 
                       show=False, feature_display_range=slice(None, -21, -1), 
                       title='[Claim number: {} || Fraud probability: {:.2f}%]'.format(pn, pr)
                      )


# In[24]:


def shap_plot(j):
    pn = shap_val_master1.iloc[[j]].claimnumber.values[0]
    pr = 100*shap_val_master1.iloc[[j]].fwa_prob.values[0]
    shap_val = shap_val_master1[[w+'_shap' for w in X_test.columns]]
    d = shap.decision_plot(explainer.expected_value, np.asarray(shap_val.iloc[[j]]), X_test.iloc[[j]], link='logit',
                           show=False, feature_display_range=slice(None, -21, -1),
                           title='[Claim number: {} || Fraud probability: {:.2f}%]'.format(pn, pr)
                           )
    plt.savefig('plots/{}.pdf'.format(pn), bbox_inches='tight')
    plt.close()
    return(d)

n = 20
for x in range(n):
    shap_plot(x)





# Smote



# In[1]:


import os
os.getcwd()
os.chdir('XX')  
cwd = os.getcwd()
cwd


# In[2]:


import pandas as pd
import numpy as np
import xgboost as xgb
from xgboost.sklearn import XGBClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
#from xgboost import plot_importance
from sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score, average_precision_score, auc
from sklearn.metrics import roc_curve, roc_auc_score, classification_report, confusion_matrix, precision_recall_curve
from sklearn.impute import SimpleImputer
from imblearn.combine import SMOTEENN
from imblearn.under_sampling import RepeatedEditedNearestNeighbours, RandomUnderSampler, NearMiss, ClusterCentroids
from imblearn.over_sampling import SMOTE
from collections import Counter
#from pandas_profiling import ProfileReport
from matplotlib import pyplot
import matplotlib.pylab as plt
get_ipython().run_line_magic('matplotlib', 'inline')
import pickle


# In[3]:


# Read data
md1 = pd.read_pickle(r"AC.pickle")


# In[4]:


def func_Target(x):
#     if x["unbun"] == 1:
#         return 1
    # "UNBUN"
    if (x['cob']+x['mul_sur']+x['chf'])>0:
        return 1
    else:
        return 0
    # "NO_WA"
md1['FWA_IND'] = md1.apply(lambda x: func_Target(x), axis = 1)

# Before running below code, please make sure data is in OHE format
data_y=pd.DataFrame()
# FWA_IND
data_y = md1['FWA_IND'].copy()
drop_variable = ['A1']

data_x = md1.drop(drop_variable, axis = 1)

drop_list2 = [
'A'
]

data_x = data_x.drop(drop_list2, axis = 1)


# #### Train test split with stratified option

# In[13]:


X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, stratify=data_y, random_state = 1234)


# In[14]:


print(y_train.value_counts())
print(y_test.value_counts())
print(sum(data_y)/data_y.shape[0])


# ### SMOTE implementation

# In[15]:


counter = Counter(y_train)
print('Original dataset shape %s' % counter)


# In[16]:


smote = NearMiss(sampling_strategy=3/7, version=3)
X_train1, y_train1 = smote.fit_resample(X_train, y_train)
counter1           = Counter(y_train1)
print('Undersampled dataset shape %s' % counter1)


# ### XGBoost cross validation

# In[17]:


def modelfit(alg, cv_folds=5, early_stopping_rounds=50):
    
    xgb_param = alg.get_xgb_params()
    xgtrain = xgb.DMatrix(X_train1.values, label=y_train1.values)
    cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,
    metrics='aucpr', early_stopping_rounds=early_stopping_rounds, verbose_eval=False)
    alg.set_params(n_estimators=cvresult.shape[0])
    
    #Fit the algorithm on the data
    alg.fit(X_train1, y_train1, eval_metric='aucpr')
        
    #Predict training set:
    y_pred_train = alg.predict(X_train1)
    y_predprob_train = alg.predict_proba(X_train1)[:,1]
    
    #Predict test set:
    y_pred_test = alg.predict(X_test)
    y_predprob_test = alg.predict_proba(X_test)[:,1]
        
    #Print model report:
    print("\nModel Report for Training set")
    print("Accuracy (Train) : %.4g" % metrics.accuracy_score(y_train1.values, y_pred_train))
    print("AUC Score (Train): %f" % metrics.roc_auc_score(y_train1, y_predprob_train))
    print("Precision Score (Train): %f" % metrics.average_precision_score(y_train1, y_predprob_train))
    print("Recall Score (Train): %f" % metrics.recall_score(y_train1, y_pred_train))
    print("\nModel Report for Test set")
    print("Accuracy (Test) : %.4g" % metrics.accuracy_score(y_test.values, y_pred_test))
    print("AUC Score (Test): %f" % metrics.roc_auc_score(y_test, y_predprob_test))
    print("Precision Score (Test): %f" % metrics.average_precision_score(y_test, y_predprob_test))
    print("Recall Score (Test): %f" % metrics.recall_score(y_test, y_pred_test))
                    
    print(pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)[0:30])


# #### Initial model with fixed parameters

# In[18]:


xgb1 = XGBClassifier(
 learning_rate=0.1,
 n_estimators=100,
 max_depth=5,
 min_child_weight=1,
 gamma=0,
 subsample=0.8,
 colsample_bytree=0.8,
 objective= 'binary:logistic',
 nthread=4,
 scale_pos_weight=1,
 max_delta_step=0,
 reg_alpha=0,
 reg_lambda=1,
 seed=2)

modelfit(xgb1)


# #### Get first 20 important variables

# In[19]:


X_train1 = X_train1[['A']]
X_test = X_test[['A1']]


# #### Fit the same model with reduced number of features

# In[20]:


xgb1 = XGBClassifier(
 learning_rate=0.1,
 n_estimators=100,
 max_depth=5,
 min_child_weight=1,
 gamma=0,
 subsample=0.8,
 colsample_bytree=0.8,
 objective= 'binary:logistic',
 nthread=4,
 scale_pos_weight=1,
 max_delta_step=0,
 reg_alpha=0,
 reg_lambda=1,
 seed=2)

modelfit(xgb1)


# #### Tune max_depth and min_child_weight

# In[21]:


param_test1 = {
 'max_depth':range(3,10,2),
 'min_child_weight':range(1,4,2)
}
gsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5, min_child_weight=1, 
                                                  gamma=0, subsample=0.8, colsample_bytree=0.8, objective='binary:logistic', 
                                                  nthread=4, scale_pos_weight=1, seed=2), 
                        param_grid=param_test1, scoring='f1', n_jobs=-1, cv=5)
gsearch1.fit(X_train1, y_train1)
gsearch1.best_params_, gsearch1.best_score_


# #### Tune gamma

# In[22]:


param_test2 = {
 'gamma':[i/10.0 for i in range(0,5)]
}
gsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=9, min_child_weight=1, 
                                                  gamma=0, subsample=0.8, colsample_bytree=0.8, objective='binary:logistic', 
                                                  nthread=4, scale_pos_weight=1, seed=2), 
                        param_grid=param_test2, scoring='f1', n_jobs=-1, cv=5)
gsearch1.fit(X_train1, y_train1)
gsearch1.best_params_, gsearch1.best_score_


# #### Use tuned parameters for an updated model

# In[23]:


xgb2 = XGBClassifier(
 learning_rate=0.1,
 n_estimators=100,
 max_depth=9,
 min_child_weight=1,
 gamma=0.0,
 subsample=0.8,
 colsample_bytree=0.8,
 objective='binary:logistic',
 nthread=4,
 scale_pos_weight=1,
 max_delta_step=0,
 reg_alpha=0,
 reg_lambda=1,
 seed=2)

modelfit(xgb2)


# #### Tune subsample and colsample_bytree

# In[24]:


param_test3 = {
 'subsample':[i/10.0 for i in range(6,10)],
 'colsample_bytree':[i/10.0 for i in range(6,10)]
}
gsearch3 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=9, min_child_weight=1, 
                                                  gamma=0.0, subsample=0.8, colsample_bytree=0.8, objective='binary:logistic', 
                                                  nthread=4, scale_pos_weight=1, seed=2), 
                        param_grid = param_test3, scoring='f1', n_jobs=-1, cv=5)
gsearch3.fit(X_train1, y_train1)
gsearch3.best_params_, gsearch3.best_score_


# #### Rerun model wih updated parameters

# In[25]:


xgb3 = XGBClassifier(
 learning_rate=0.1,
 n_estimators=100,
 max_depth=9,
 min_child_weight=1,
 gamma=0.0,
 subsample=0.9,
 colsample_bytree=0.9,
 objective='binary:logistic',
 nthread=4,
 scale_pos_weight=9,
 max_delta_step=1,
 reg_alpha=0,
 reg_lambda=1,
 seed=2)

modelfit(xgb3)


# In[26]:


xgb4 = XGBClassifier(
 learning_rate=0.1,
 n_estimators=100,
 max_depth=9,
 min_child_weight=1,
 gamma=0.0,
 subsample=0.9,
 colsample_bytree=0.9,
 objective='binary:logistic',
 nthread=4,
 scale_pos_weight=9,
 max_delta_step=1,
 reg_alpha=0,
 reg_lambda=1,
 seed=2)

xgb4.fit(X_train1, y_train1)


# ### Model results

# In[27]:


y_pred = (xgb4.predict_proba(X_test)[:,1] >= 0.5).astype(bool)

print('F1 Accuracy : {}'.format(f1_score(y_test, y_pred)))
print('Precision Score: {}'.format(precision_score(y_test, y_pred)))
print('Recall Score: {}'.format(recall_score(y_test, y_pred)))
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))


# In[ ]:



# from sklearn.model_selection import RandomizedSearchCV
# import xgboost as xgb




from sklearn.metrics import precision_score, recall_score, accuracy_score
from sklearn.metrics import roc_auc_score, make_scorer
from sklearn.model_selection import train_test_split
import xgboost as xgb
import numpy as np
from sklearn.model_selection import RandomizedSearchCV
import pandas as pd

md1 = pd.read_pickle(r"C:\Data\MM\features_dataset_0901_v1.pickle")
def func_Target(x):
    if x["unbun"] == 1:
        return "UNBUN"
    elif (x['cob']+x['mul_sur'])>0:
        return "COB_MUL"
    else:
        return "NO_WA"
md1['FWA_IND'] = md1.apply(lambda x: func_Target(x), axis = 1)


# Before running below code, please make sure data is in OHE format
data_y=pd.DataFrame()
# FWA_IND
data_y = md1['Y'].copy()
data_x = md1.drop(['Y'], axis = 1)
# data_x = md1.drop(['unbun','cob','mul_sur', 'FWA_IND'])

X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, 
                                                    test_size = 0.20, 
                                                    random_state = 0)
n_estimators = [250]
# ,300,500,700,800,1000
learning_rate = [0.01]
# ,0.02,0.05,0.08,0.1
max_depth = np.arange(2,14,2)
subsample = [0.9]
# 0.7,0.8,
colsample_bytree = [0.9]
# 0.7,0.8,
alpha = [0]
# ,0.5,1,1.5
booster = ['gbtree']
min_child_weight = [1,2,3,4,5,7]
grid_param = {'n_estimators': n_estimators,
               'learning_rate':learning_rate,
               'max_depth':max_depth,
               'subsample':subsample,
               'colsample_bytree':colsample_bytree,
               'alpha':alpha,
               'booster':booster,
               'min_child_weight':min_child_weight}

XGB = xgb.XGBClassifier(objective = 'binary:logistic')

# numFolds = 5
# kfold_5 = cross_validation.KFold(n = len(X), shuffle = True, 
#                                  n_folds = numFolds)
scoring_evals = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score), 
                 'Precision': make_scorer(precision_score),
                 'Recall': make_scorer(recall_score)}

xgb_param_tunning = RandomizedSearchCV(estimator= XGB,
                                       param_distributions= grid_param,
                                       n_iter = 100,
                                       cv = 5,
                                       scoring = 'roc_auc',
                                       verbose = 2, 
                                       random_state=42)
xgb_param_tunning.fit(X_train,y_train)
print(xgb_param_tunning.best_params_)
print(xgb_param_tunning.best_score_)

pd.DataFrame(xgb_param_tunning.cv_results_).to_csv(r"C:\Data\RandomSearch_Output\Random_Search_Output1.csv")


# Feature Importance metrics
# Use parameters from the random search 
import xgboost as xgb

XGB = xgb.XGBClassifier(objective = 'binary:logistic', subsample = 0.7, n_estimators = 700,
                        min_child_weight = 3.0, max_depth = 4, learning_rate = 0.01, 
                        colsample_bytree = 0.7, booster = 'gbtree', alpha = 0.5, 
                        random_state=42, verbosity = 1, tree_method = 'exact')

gbm = XGB.fit(X_train, y_train)

y_pred_test = gbm.predict(X_test)
y_pred_train = gbm.predict(X_train)

from sklearn.inspection import permutation_importance
r= permutation_importance(gbm, X_test, y_test, n_repeats=15,random_state=0)

col = pd.DataFrame(X_test.columns, columns = ["Features"])
col.reset_index(inplace = True)

feature_imp = pd.DataFrame()
feature_imp["Features"]=col.Features
feature_imp["index"]=col.index
importance = pd.Series(r.importances_mean)
std_dev = pd.Series(r.importances_std)

feature_imp = feature_imp.merge(importance.rename('importance'),
                                left_on = "index", right_index=True)
feature_imp = feature_imp.merge(std_dev.rename('std_dev'),
                                left_on = "index", right_index=True) 

feature_imp.drop(["index"], axis = 1, inplace = True)





import xgboost as xgx
xg = xgx.XGBClassifier(
    objective = 'binary:logistic',
    max_depth = 4,
    n_estimators = 700,
    scale_pos_weight = 1200,
    learning_rate = 0.05,
    n_jobs=-1,
    random_state=101)
get_ipython().run_line_magic('time', 'xg.fit(X_train,y_train)')

#Defining the model
Model41_Var31 = xg # Model name instead of xg

# Function to create a Xgboost model with parameters given by user
def Xgboost_Model_Function2(target_variable_name,var_to_be_dropped,
                            iterations,depth,learning_rate,weights,inc_ntimes,
                            from_ntree,inc_by):
    print("FUNCTION STARTED RUNNING")
    model1=Model41_Var31
    print("MODEL DEFINED")
     ## Codes for KS
    def ks(data=None,target=None, prob=None):
        data['target0'] = 1 - data[target]
        data['bucket'] = np.where(
                data[prob] >= Hard_Decile_GB.iloc[0,1], 1, np.where(
                data[prob] >= Hard_Decile_GB.iloc[1,1], 2, np.where(
                data[prob] >= Hard_Decile_GB.iloc[2,1], 3, np.where(
                data[prob] >= Hard_Decile_GB.iloc[3,1], 4, np.where(
                data[prob] >= Hard_Decile_GB.iloc[4,1], 5, np.where(
                data[prob] >= Hard_Decile_GB.iloc[5,1], 6, np.where(
                data[prob] >= Hard_Decile_GB.iloc[6,1], 7, np.where(
                data[prob] >= Hard_Decile_GB.iloc[7,1], 8, np.where(
                data[prob] >= Hard_Decile_GB.iloc[8,1], 9, 10)))))))))               
        grouped = data.groupby('bucket', as_index = False)
        kstable = pd.DataFrame()
        kstable['min_prob'] = grouped.min()[prob]
        kstable['max_prob'] = grouped.max()[prob]
        kstable['events']   = grouped.sum()[target]
        kstable['nonevents'] = grouped.sum()['target0']
        kstable = kstable.sort_values(by="min_prob", ascending=False).reset_index(drop = True)
        kstable['event_rate'] = (kstable.events / data[target].sum())
        kstable['nonevent_rate'] = (kstable.nonevents / data['target0'].sum())
        kstable['cum_eventrate']=(kstable.events / data[target].sum()).cumsum()
        kstable['cum_noneventrate']=(kstable.nonevents / data['target0'].sum()).cumsum()
        kstable['KS'] = np.round(kstable['cum_eventrate']-kstable['cum_noneventrate'], 3) * 100
        #Formating
        kstable['cum_eventrate']= kstable['cum_eventrate']
        kstable['cum_noneventrate']= kstable['cum_noneventrate']
        kstable.index = range(1,11)
        kstable.index.rename('Decile', inplace=True)
        pd.set_option('display.max_columns', 9)
        print(kstable)
        
        #Display KS
        from colorama import Fore
        print(Fore.RED + "KS is " + str(max(kstable['KS']))+"%"+ " at decile " + str((kstable.index[kstable['KS']==max(kstable['KS'])][0])))
        return(kstable)


    print("KS FUNCTION DEFINED")
    # Model Validation

    from sklearn.metrics import balanced_accuracy_score, roc_auc_score, make_scorer
    x=inc_ntimes
    from_ntree=from_ntree
    inc_by=inc_by
    
    ks1_train=[]
    auc_train=[]
    decile_train=[]
    Break_Event_Rate_train=[]
    Lift_train=[]
    Event_01_train=[]
    Event_02_train=[]
    Event_03_train=[]
    Event_04_train=[]
    Event_05_train=[]
    Event_06_train=[]
    Event_07_train=[]
    Event_08_train=[]
    Event_09_train=[]
    Event_10_train=[]
    NonEvent_01_train=[]
    NonEvent_02_train=[]
    NonEvent_03_train=[]
    NonEvent_04_train=[]
    NonEvent_05_train=[]
    NonEvent_06_train=[]
    NonEvent_07_train=[]
    NonEvent_08_train=[]
    NonEvent_09_train=[]
    NonEvent_10_train=[]
    
    ks1_test=[]
    auc_test=[]
    decile_test=[]
    Break_Event_Rate_test=[]
    Lift_test=[]
    Event_01_test=[]
    Event_02_test=[]
    Event_03_test=[]
    Event_04_test=[]
    Event_05_test=[]
    Event_06_test=[]
    Event_07_test=[]
    Event_08_test=[]
    Event_09_test=[]
    Event_10_test=[]
    NonEvent_01_test=[]
    NonEvent_02_test=[]
    NonEvent_03_test=[]
    NonEvent_04_test=[]
    NonEvent_05_test=[]
    NonEvent_06_test=[]
    NonEvent_07_test=[]
    NonEvent_08_test=[]
    NonEvent_09_test=[]
    NonEvent_10_test=[]
   
    
    #ks1_oot=[]
    #auc_oot=[]
    #decile_oot=[]
    #Break_Event_Rate_oot=[]
    #Lift_oot=[]
    
    iteration=[]
    print("BLANK SERIES CREATED, NEXT --> LOOP WILL BEGIN")
    for i in range(x):
        try:
            df_target1 = y_train.copy()
            df_target1.columns = ['TARGET']
            print("COPY OF Y TRAIN CREATED, NEXT --> SCORING TO BEGIN")
            df_target1['Prob']=model1.predict_proba(X_train,ntree_limit=from_ntree)[:,1]
            print("SCORING COMPLETED ON y train for HARD DECILE DEFINITION")
            df_target1['Hard_Decile'] = pd.qcut(df_target1.Prob, 10, labels=[10,9,8,7,6,5,4,3,2,1])        
        
            #Definition of hard deciles
            Hard_Decile_GB = df_target1[['Prob']].groupby([df_target1.Hard_Decile]).agg(['min'])
            Hard_Decile_GB = Hard_Decile_GB.reset_index()
            Hard_Decile_GB.columns = ['Hard_Decile','Min_Prob']
            Hard_Decile_GB = Hard_Decile_GB.sort_values(by="Min_Prob", ascending=False)
            print("HARD DECILE GB CREATED")
           
            
            # Train
            df_X_train1=y_train.copy()
            df_X_train1['Prob']=df_target1['Prob']
            del df_target1
            mydf_train = ks(data=df_X_train1,target=target_variable_name, prob='Prob')
            mydf_train.to_csv('Hard_Decile_result_train.csv')
            print("TRAIN DATA KS DONE, NEXT --> TEST1 SCORING TO BEGIN")
      
            # Test
            df_X_test1=y_test[[target_variable_name]]    
            temp=model1.predict_proba(X_test,ntree_limit=from_ntree)
            print("TEST 1 SCORING DONE")
            df_X_test1['Prob']=temp[:,1]
            mydf_test1 = ks(data=df_X_test1,target=target_variable_name, prob="Prob")
            mydf_test1.to_csv('Hard_Decile_result_test.csv')
            print("TEST1 DATA KS DONE, NEXT --> TEST2 SCORING TO BEGIN")
           
          
            # OOT
            #df_X_oot1=y_oot[[target_variable_name]]    
            #temp2=model1.predict_proba(X_oot,ntree_limit=from_ntree)
            #print("OOT SCORING DONE")
            #df_X_oot1['Prob']=temp2[:,1]
            #mydf_oot = ks(data=df_X_oot1,target=target_variable_name, prob="Prob")
            #print("OOT DATA KS DONE, NEXT --> KS APPEND FOR ALL DATA")
        
            ks1_train.append(max(mydf_train.KS))
            ks1_test.append(max(mydf_test1.KS))
            #ks1_oot.append(max(mydf_oot.KS))
            print("KS APPENDED, NEXT --> AUC APPEND")
        
        
            auc_train.append(roc_auc_score(y_train[target_variable_name],df_X_train1.Prob))
            auc_test.append(roc_auc_score(y_test[target_variable_name],df_X_test1.Prob))
            #auc_oot.append(roc_auc_score(y_oot[target_variable_name],df_X_oot1.Prob))
            print("AUC APPENDED, NEXT --> KS-DECILE APPEND")
            
        
            #KS-Decile
            decile_train.append(mydf_train.index[mydf_train['KS']==max(mydf_train['KS'])][0])
            decile_test.append(mydf_test1.index[mydf_test1['KS']==max(mydf_test1['KS'])][0])
            #decile_oot.append(mydf_oot.index[mydf_oot['KS']==max(mydf_oot['KS'])][0])
            print("KS-DECILE APPENDED, NEXT --> BREAK APPEND")
            
            #KS - Break in Event Rate - Decile
            Break_Event_Rate_train.append(np.where(mydf_train.event_rate.iloc[1] > mydf_train.event_rate.iloc[0],2, np.where(mydf_train.event_rate.iloc[2] > mydf_train.event_rate.iloc[1],3, np.where(mydf_train.event_rate.iloc[3] > mydf_train.event_rate.iloc[2],4, np.where(mydf_train.event_rate.iloc[4] > mydf_train.event_rate.iloc[3],5, np.where(mydf_train.event_rate.iloc[5] > mydf_train.event_rate.iloc[4],6, np.where(mydf_train.event_rate.iloc[6] > mydf_train.event_rate.iloc[5],7, np.where(mydf_train.event_rate.iloc[7] > mydf_train.event_rate.iloc[6],8, np.where(mydf_train.event_rate.iloc[8] > mydf_train.event_rate.iloc[7],9, np.where(mydf_train.event_rate.iloc[9] > mydf_train.event_rate.iloc[8],10,"NO"))))))))))
            Break_Event_Rate_test.append(np.where(mydf_test1.event_rate.iloc[1] > mydf_test1.event_rate.iloc[0],2, np.where(mydf_test1.event_rate.iloc[2] > mydf_test1.event_rate.iloc[1],3, np.where(mydf_test1.event_rate.iloc[3] > mydf_test1.event_rate.iloc[2],4, np.where(mydf_test1.event_rate.iloc[4] > mydf_test1.event_rate.iloc[3],5, np.where(mydf_test1.event_rate.iloc[5] > mydf_test1.event_rate.iloc[4],6, np.where(mydf_test1.event_rate.iloc[6] > mydf_test1.event_rate.iloc[5],7, np.where(mydf_test1.event_rate.iloc[7] > mydf_test1.event_rate.iloc[6],8, np.where(mydf_test1.event_rate.iloc[8] > mydf_test1.event_rate.iloc[7],9, np.where(mydf_test1.event_rate.iloc[9] > mydf_test1.event_rate.iloc[8],10,"NO"))))))))))
            #Break_Event_Rate_oot.append(np.where(mydf_oot.event_rate.iloc[1] > mydf_oot.event_rate.iloc[0],2, np.where(mydf_oot.event_rate.iloc[2] > mydf_oot.event_rate.iloc[1],3, np.where(mydf_oot.event_rate.iloc[3] > mydf_oot.event_rate.iloc[2],4, np.where(mydf_oot.event_rate.iloc[4] > mydf_oot.event_rate.iloc[3],5, np.where(mydf_oot.event_rate.iloc[5] > mydf_oot.event_rate.iloc[4],6, np.where(mydf_oot.event_rate.iloc[6] > mydf_oot.event_rate.iloc[5],7, np.where(mydf_oot.event_rate.iloc[7] > mydf_oot.event_rate.iloc[6],8, np.where(mydf_oot.event_rate.iloc[8] > mydf_oot.event_rate.iloc[7],9, np.where(mydf_oot.event_rate.iloc[9] > mydf_oot.event_rate.iloc[8],10,"NO"))))))))))
            print("BREAK APPENDED, NEXT --> LIFT APPEND")
            
            #Lift - Decile-1
            
            Lift_train.append((mydf_train.events.iloc[0]/(mydf_train.events.iloc[0] + mydf_train.nonevents.iloc[0]))/ (mydf_train.events.sum()/(mydf_train.events.sum() + mydf_train.nonevents.sum())))
            Lift_test.append((mydf_test1.events.iloc[0]/(mydf_test1.events.iloc[0] + mydf_test1.nonevents.iloc[0]))/ (mydf_test1.events.sum()/(mydf_test1.events.sum() + mydf_test1.nonevents.sum())))
            #Lift_oot.append((mydf_oot.events.iloc[0]/(mydf_oot.events.iloc[0] + mydf_oot.nonevents.iloc[0]))/ (mydf_oot.events.sum()/(mydf_oot.events.sum() + mydf_oot.nonevents.sum())))
            
            print("LIFT APPENDED, NEXT --> Event and NonEvent Decile wise Counts")
            
            Event_01_train.append(mydf_train.events.iloc[0])
            Event_02_train.append(mydf_train.events.iloc[1])
            Event_03_train.append(mydf_train.events.iloc[2])
            Event_04_train.append(mydf_train.events.iloc[3])
            Event_05_train.append(mydf_train.events.iloc[4])
            Event_06_train.append(mydf_train.events.iloc[5])
            Event_07_train.append(mydf_train.events.iloc[6])
            Event_08_train.append(mydf_train.events.iloc[7])
            Event_09_train.append(mydf_train.events.iloc[8])
            Event_10_train.append(mydf_train.events.iloc[9])
            NonEvent_01_train.append(mydf_train.nonevents.iloc[0])
            NonEvent_02_train.append(mydf_train.nonevents.iloc[1])
            NonEvent_03_train.append(mydf_train.nonevents.iloc[2])
            NonEvent_04_train.append(mydf_train.nonevents.iloc[3])
            NonEvent_05_train.append(mydf_train.nonevents.iloc[4])
            NonEvent_06_train.append(mydf_train.nonevents.iloc[5])
            NonEvent_07_train.append(mydf_train.nonevents.iloc[6])
            NonEvent_08_train.append(mydf_train.nonevents.iloc[7])
            NonEvent_09_train.append(mydf_train.nonevents.iloc[8])
            NonEvent_10_train.append(mydf_train.nonevents.iloc[9])
            
            Event_01_test.append(mydf_test1.events.iloc[0])
            Event_02_test.append(mydf_test1.events.iloc[1])
            Event_03_test.append(mydf_test1.events.iloc[2])
            Event_04_test.append(mydf_test1.events.iloc[3])
            Event_05_test.append(mydf_test1.events.iloc[4])
            Event_06_test.append(mydf_test1.events.iloc[5])
            Event_07_test.append(mydf_test1.events.iloc[6])
            Event_08_test.append(mydf_test1.events.iloc[7])
            Event_09_test.append(mydf_test1.events.iloc[8])
            Event_10_test.append(mydf_test1.events.iloc[9])
            NonEvent_01_test.append(mydf_test1.nonevents.iloc[0])
            NonEvent_02_test.append(mydf_test1.nonevents.iloc[1])
            NonEvent_03_test.append(mydf_test1.nonevents.iloc[2])
            NonEvent_04_test.append(mydf_test1.nonevents.iloc[3])
            NonEvent_05_test.append(mydf_test1.nonevents.iloc[4])
            NonEvent_06_test.append(mydf_test1.nonevents.iloc[5])
            NonEvent_07_test.append(mydf_test1.nonevents.iloc[6])
            NonEvent_08_test.append(mydf_test1.nonevents.iloc[7])
            NonEvent_09_test.append(mydf_test1.nonevents.iloc[8])
            NonEvent_10_test.append(mydf_test1.nonevents.iloc[9])
            
            print("Event and NonEvent Decile wise Counts APPENDED, NEXT --> ITERATION CHANGE")
            
            
            
            iteration.append(from_ntree)
            from_ntree=from_ntree+inc_by
            print(i)
        except:
            from_ntree=from_ntree+inc_by
            print(i)
    
    print("LOOP COMPLETE, NEXT -->COMBINING ALL METRICS IN ONE DATAFRAME")
    validation1=pd.DataFrame()
    validation1['ks_train']=pd.Series(ks1_train)
    validation1['auc_train']=pd.Series(auc_train)
    validation1['decile_train']=pd.Series(decile_train)
    validation1['Break_Event_Rate_train']=pd.Series(Break_Event_Rate_train)
    validation1['Lift_train']=pd.Series(Lift_train)
    
    validation1['Event_01_train']=pd.Series(Event_01_train)
    validation1['Event_02_train']=pd.Series(Event_02_train)
    validation1['Event_03_train']=pd.Series(Event_03_train)
    validation1['Event_04_train']=pd.Series(Event_04_train)
    validation1['Event_05_train']=pd.Series(Event_05_train)
    validation1['Event_06_train']=pd.Series(Event_06_train)
    validation1['Event_07_train']=pd.Series(Event_07_train)
    validation1['Event_08_train']=pd.Series(Event_08_train)
    validation1['Event_09_train']=pd.Series(Event_09_train)
    validation1['Event_10_train']=pd.Series(Event_10_train)
    
    validation1['NonEvent_01_train']=pd.Series(NonEvent_01_train)
    validation1['NonEvent_02_train']=pd.Series(NonEvent_02_train)
    validation1['NonEvent_03_train']=pd.Series(NonEvent_03_train)
    validation1['NonEvent_04_train']=pd.Series(NonEvent_04_train)
    validation1['NonEvent_05_train']=pd.Series(NonEvent_05_train)
    validation1['NonEvent_06_train']=pd.Series(NonEvent_06_train)
    validation1['NonEvent_07_train']=pd.Series(NonEvent_07_train)
    validation1['NonEvent_08_train']=pd.Series(NonEvent_08_train)
    validation1['NonEvent_09_train']=pd.Series(NonEvent_09_train)
    validation1['NonEvent_10_train']=pd.Series(NonEvent_10_train)
    
    
    validation1['ks_test1']=pd.Series(ks1_test)
    validation1['auc_test']=pd.Series(auc_test)
    validation1['decile_test']=pd.Series(decile_test)
    validation1['Break_Event_Rate_test']=pd.Series(Break_Event_Rate_test)
    validation1['Lift_test']=pd.Series(Lift_test)
    
    validation1['Event_01_test']=pd.Series(Event_01_test)
    validation1['Event_02_test']=pd.Series(Event_02_test)
    validation1['Event_03_test']=pd.Series(Event_03_test)
    validation1['Event_04_test']=pd.Series(Event_04_test)
    validation1['Event_05_test']=pd.Series(Event_05_test)
    validation1['Event_06_test']=pd.Series(Event_06_test)
    validation1['Event_07_test']=pd.Series(Event_07_test)
    validation1['Event_08_test']=pd.Series(Event_08_test)
    validation1['Event_09_test']=pd.Series(Event_09_test)
    validation1['Event_10_test']=pd.Series(Event_10_test)
    
    validation1['NonEvent_01_test']=pd.Series(NonEvent_01_test)
    validation1['NonEvent_02_test']=pd.Series(NonEvent_02_test)
    validation1['NonEvent_03_test']=pd.Series(NonEvent_03_test)
    validation1['NonEvent_04_test']=pd.Series(NonEvent_04_test)
    validation1['NonEvent_05_test']=pd.Series(NonEvent_05_test)
    validation1['NonEvent_06_test']=pd.Series(NonEvent_06_test)
    validation1['NonEvent_07_test']=pd.Series(NonEvent_07_test)
    validation1['NonEvent_08_test']=pd.Series(NonEvent_08_test)
    validation1['NonEvent_09_test']=pd.Series(NonEvent_09_test)
    validation1['NonEvent_10_test']=pd.Series(NonEvent_10_test)
    
    
    #validation1['ks_oot']=pd.Series(ks1_oot)
    #validation1['auc_oot']=pd.Series(auc_oot)
    #validation1['decile_oot']=pd.Series(decile_oot)
    #validation1['Break_Event_Rate_oot']=pd.Series(Break_Event_Rate_oot)
    #validation1['Lift_oot']=pd.Series(Lift_oot)
    
    validation1['Iteration']=pd.Series(iteration)

    validation1['Model']='XGB'
    validation1['Depth']=depth
    validation1['Total_Iterations']=iterations
    validation1['Learning_Rate']=learning_rate

    validation1=validation1[['Model','Depth','Total_Iterations','Learning_Rate','Iteration','ks_train','ks_test1','auc_train','auc_test','decile_train','decile_test','Break_Event_Rate_train','Break_Event_Rate_test','Lift_train','Lift_test','Event_01_train','Event_02_train','Event_03_train','Event_04_train','Event_05_train','Event_06_train','Event_07_train','Event_08_train','Event_09_train','Event_10_train','NonEvent_01_train','NonEvent_02_train','NonEvent_03_train','NonEvent_04_train','NonEvent_05_train','NonEvent_06_train','NonEvent_07_train','NonEvent_08_train','NonEvent_09_train','NonEvent_10_train','Event_01_test','Event_02_test','Event_03_test','Event_04_test','Event_05_test','Event_06_test','Event_07_test','Event_08_test','Event_09_test','Event_10_test','NonEvent_01_test','NonEvent_02_test','NonEvent_03_test','NonEvent_04_test','NonEvent_05_test','NonEvent_06_test','NonEvent_07_test','NonEvent_08_test','NonEvent_09_test','NonEvent_10_test']]

    validation1.to_csv('Model41_Var31.csv',index=False)
    print("VALIDATION DATAFRAME WRITTEN TO THE PATH")



# Put target variable name
target_variable_name='policy_sold' #Target Variable name - MM

var_to_be_dropped=[] # If no varaibles are to be dropped from the model

iterations=700
depth=4
learning_rate=0.05
weights=1200
inc_ntimes=2
from_ntree=250
inc_by=100

# Calling the function
Xgboost_Model_Function2(target_variable_name,var_to_be_dropped,iterations,depth,learning_rate,weights,inc_ntimes,from_ntree,inc_by) 



# Lag varuable
Temp11 = All_data['V1','V2']].head(100000)

# Variables for past n days
from datetime import timedelta, datetime

 

for lim in [30,60]:
    li=[]
    li2 = []
    def func_past_days(x):
        x=x.sort_values("IncurredFromDate")  #sort on the basis on Date variable
        li.append(None)
        li2.append(None)
        for i in range(1,len(x)):
            y=x.reset_index(drop = True).loc[:i-1]
            date = x.reset_index(drop = True).loc[i,'IncurredFromDate']-timedelta(days = lim)
            li.append(y[y['IncurredFromDate']>=date]['TotalCharge'].sum())
            li2.append(y[y['IncurredFromDate']>=date]['TotalIneligibleAmt'].sum())
    
    Temp11.groupby(['EnrolleeSequenceNumber',
                                       'MemberCode']).apply(lambda x: func_past_days(x)).reset_index(inplace = True)
    Temp11 = Temp11.sort_values(['EnrolleeSequenceNumber','MemberCode','IncurredFromDate'])
    Temp11['Total_Charge_'+str(lim)]=li # Name of new variable
    Temp11['TotalIneligibleAmt_'+str(lim)]=li2 # Name of new variable
    Temp11.reset_index(drop=True, inplace = True)




